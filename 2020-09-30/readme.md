## Snapshot of performance benchmark data, 2020-09-30

Run with commit https://github.com/Agoric/agoric-sdk/commit/8a21a315a33ce187200c20a54ac08443d420ae3f

These data are intended to help us pin down the performance costs of various
aspects of the SwingSet kernel and liveslots, in order to make prioritization
decisions about the various bits of work we know we still need to do.  In
particular, we want to tease out scaling or performance issues that we'd
consider blockers for rolling out a mainnet.

All the data here is generated by executing various different tests from the
Swingset Runner `demos` directory.  Swingset Runner produces two kinds of
measurement output.  One is a summary report generated at the end of execution
that presents the cumulative tallies from the various counters in the kernel's
internal instrumentation (in the case of explicit multi-round benchmarks, each
run actually generates two different reports of this sort, one for the setup
phase and one for the aggregate benchmark execution).  The second is a set of
stats tables reporting measurements taken by Swingset Runner itself.  These
measure things like execution time and memory usage; these latter data sets are
relatively large, as a report record is generated for each execution block (a
block, in this case, is simply a defined number of cranks; for these tests the
block size was set to 250).  These data are more readily perceived by graphing
them rather than by producing a summary report, as they are primarily useful for
seeing how various performance metrics vary over time (in particular, how they
degrade over time as resources are consumed).


Broadly speaking there are two sets of test results found here, corresponding to
the above reporting dichotomy.  The first set ("Demo Reports") is derived from
simply executing each of the defined `demo` swingsets for each of the test cases
it supports.  These tests are all short to very short (they range from 4 to 345
cranks), but can give a sense of the relative costs of various different
operations.  Summary reports are provided for these, but not stats graphs as
they are too short (and too heterogeneous) to yield much useful aggregate data.
The second set ("Benchmark Data") is derived from running multiple benchmark
rounds of three different benchmark swingsets.  For these, both summary reports
and resource consumption graphs are provided, as well as the raw data used to
generate the latter (in case you want to import the numbers into a spreadsheet
and crunch them in some different way).

### Demo Reports

There are 41 demo reports.  Since they are short, they are all collected in one
file, `demo-reports`.  For each we show the command line that was executed, the
table of kernel counter data, and summary crank and timing numbers.

### Benchmark Data

The benchmark data were derived from running three benchmark tests that consist
of an interaction between two vats, Alice and Bob, each round.

In the PingPong test, in each round Alice sends a ping message to Bob and Bob
responds with a ping message to Alice.  In the Swap and Exchange tests,
Alice and Bob trade moola for simoleans and then trade them back again.  The
Swap test uses the atomic swap contract, which gets repeatedly instantiated
for each round, whereas the Exchange test uses the exchange contract, where a
single contract is used throughout the entire run.

The original plan was to run each test for 100, 1,000, and 10,000 rounds.  The
1,000 and 10,000 round tests were intended to calibrate whether the 100 round
test can be used as a reliable measure of per-round cost -- obviously it can't
be if the cost-per-round increases over time.  Each of these tests was in turn
run twice, both with and without the Swingset Runner `--forcegc` flag set.  This
flag causes a GC to be run at the end of each crank.  Running without forced GC
gives good aggregate performance numbers, but running with forced GC provides
more accurate marginal memory usage numbers.

As a practical matter, attempting to run the Exchange test for a large number of
rounds failed, as its RAM footprint grows over time and starts to get into heavy
GC thrashing (this is on NodeJS running with its default heap limit, around
1.7G).  The 1,000 round test took a little over two minutes, but I when ran the
10,000 round test overnight, in the morning I found it had only gotten to around
5500 rounds in just over 9 hours of clock time.  To get maximal performance
numbers, I reran it, setting the upper bound to 4250 rounds, which takes about a
half hour (on round 4295 it goes into a long GC pause that lasts almost an
hour, motivating the cutoff point I chose).  This is unfortunate because the
Exchange test is probably the closest analog to some of our anticipated
production use cases.

The elapsed time for the 1,000 round Swap test was slightly less than exactly 10 times the
elapsed time for the 100 round Swap test, but since the 1,000 round test took 22
and a half minutes, the 10,000 test would take almost 4 hours so I didn't
bother.

Summaring the times for the 8 basic tests (without forced GC):

Test            | Total (ns)      | Total (legible) | Per round (ns) | Per round (legible)
----------------|-----------------|-----------|------------|-------
PingPong 100    | 559044958       | 559ms     | 559044     | 5.6ms
PingPong 1,000  | 4748436319      | 4.748s    | 47484363   | 4.7ms
PingPong 10,000 | 47111846536     | 47.112s   | 47111846   | 4.7ms
Exchange 100    | 13350992965     | 13.351s   | 133509929  | 133ms
Exchange 1,000  | 144333817241    | 2:24.334  | 144333817  | 144ms
Exchange 4,250  | 1764395904860   | 29:24.396 | 415151977  | 415ms
Swap 100        | 133846177256    | 2:13.846  | 1338461772 | 1.338s
Swap 1,000      | 1355564942124   | 22:35.564 | 1355564942 | 1.356s

The tests were run (and the graphs generated) using the `runem` script, which is
included in this repository along with the data.

The eight basic tests run are labeled `exchange`, `exchange1k`, `exchange4k`,
`swap`, `swap1k`, `pingpong`, `pingpong1k`, and `pingpong10k`, with what I hope
are the obvious meanings.  The corresponding tests with the `--forcegc` flag
enabled are labeled `TESTNAMEgc`, e.g., `swap1kgc`.

For each test, the files produced are:

Filename                    | Meaning
:---------------------------|:-----------------------------
`stats-TESTNAME`            | the stats data file output by `bin/swingset-runner`
`TESTNAME-benchmark.report` | the summary report
`TESTNAME-benchmark.log`    | the raw execution log
`TESTNAMEDisk.pdf`          | graph of disk usage over time, as a PDF
`TESTNAMEDisk.png`          | graph of disk usage over time, as a PNG
`TESTNAMEMem.pdf`           | graph of memory usage over time, as a PDF
`TESTNAMEMem.png`           | graph of memory usage over time, as a PNG
`TESTNAMETime.pdf`          | graph of execution time, as a PDF
`TESTNAMETime.png`          | graph o fexecution time, as a PNG

Graphs are provided in both PDF and PNG format.  PNGs are more suitable for
embedding in something like a web page, while PDFs are in vector format that lets
you zoom in on the data in a PDF viewer.
